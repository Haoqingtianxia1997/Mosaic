# -*- coding: utf-8 -*-
"""
intention_predict.py
å®æ—¶è§†é¢‘æµï¼šäººä½“å§¿æ€ + å¤´éƒ¨æœå‘ ROI + YOLO ç›®æ ‡æ£€æµ‹ + è¯­éŸ³äº¤äº’
"""
import cv2                      # OpenCV å›¾åƒå¤„ç†ä¸æ‘„åƒå¤´
import time                     # æ—¶é—´å‡½æ•°
import mediapipe as mp          # Google MediaPipe å§¿æ€/åˆ†å‰²
import numpy as np              # æ•°å€¼è¿ç®—
import sys, os                  # ç³»ç»Ÿç›¸å…³
from ultralytics import YOLO    # Ultralytics YOLOv8
# from src.mistral_ai.label_analyzer import LabelAnalyzer  # å¯é€‰ï¼šLLM æ ‡ç­¾åˆ†æ
from src.transcribe.tts import play_text_to_speech        # æ–‡æœ¬è½¬è¯­éŸ³
from src.transcribe.stt import VoiceTranscriber           # è¯­éŸ³è½¬æ–‡æœ¬


def intention_predict(model_path) -> None:
    """
    ä¸»å…¥å£ï¼šæ‰“å¼€æ‘„åƒå¤´ï¼Œå®æ—¶æ¨ç†äººä½“å§¿åŠ¿ã€å¤´éƒ¨æ–¹å‘ã€ç›®æ ‡æ£€æµ‹å¹¶è¯­éŸ³äº¤äº’
    """
    # åˆå§‹åŒ–è¯­éŸ³è½¬å†™å™¨å’Œ YOLOv8 æ¨¡å‹
    model_path = model_path if model_path else 'yolo_model/yolo11m.pt'  # é»˜è®¤æƒé‡è·¯å¾„
    transcriber = VoiceTranscriber()
    model = YOLO(model_path)               # è‡ªå·±è®­ç»ƒå¥½çš„æƒé‡
    label_times = {}                         # {label: last_seen_time}
    window_seconds = 15                       # æ ‡ç­¾ä¿ç•™æ—¶é—´çª—å£ (ç§’)
    tts_interval = 30                        # æ¯æ¬¡ TTS/STT é—´éš” (ç§’)
    detected_labels = set()                  # å½“å‰çª—å£å†…æ£€æµ‹åˆ°çš„æ ‡ç­¾
    last_query_result = None                 # ä¸Šä¸€æ¬¡ STT è§£æç»“æœ

    # åˆå§‹åŒ– MediaPipe Pose ä¸ SelfieSegmentation
    mp_pose = mp.solutions.pose
    pose = mp_pose.Pose(
        static_image_mode=False,
        model_complexity=1,
        enable_segmentation=False
    )
    mp_selfie_segmentation = mp.solutions.selfie_segmentation
    selfie_segmentation = mp_selfie_segmentation.SelfieSegmentation(
        model_selection=1      # 1=ç²¾åº¦é«˜ä½†è¾ƒæ…¢ï¼›0=é€Ÿåº¦å¿«é²æ£’æ€§é«˜
    )

    # æ‰“å¼€é»˜è®¤æ‘„åƒå¤´ (ç´¢å¼• 0)
    capture = cv2.VideoCapture(0)
    # label_analyzer = LabelAnalyzer()       # è‹¥éœ€è¦ LLM è¿›ä¸€æ­¥åˆ†æ

    tts_stt_in_progress = False              # æ˜¯å¦æ­£åœ¨æ‰§è¡Œ TTS/STT
    last_tts_time = 0                        # ä¸Šæ¬¡ TTS æ—¶é—´æˆ³

    # ===== ä¸»å¾ªç¯ï¼šé€å¸§å¤„ç† =====
    while capture.isOpened():
        ret, frame = capture.read()          # è¯»å–ä¸€å¸§
        if not ret:                          # è¯»å–å¤±è´¥åˆ™é€€å‡º
            break

        current_time = time.time()           # å½“å‰æ—¶é—´æˆ³
        h, w, _ = frame.shape                # å›¾åƒé«˜ã€å®½ã€é€šé“

        # å®æ—¶æ˜¾ç¤ºåŸå§‹å¸§
        cv2.imshow('Head Pose + Local YOLO Detection', frame)

        # å¦‚æœæ­£åœ¨è¯­éŸ³äº¤äº’ï¼Œæš‚åœæ£€æµ‹ï¼Œåªæ˜¾ç¤ºæç¤º
        if tts_stt_in_progress:
            cv2.putText(
                frame, "Paused for TTS/STT...", (50, 200),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2
            )
            cv2.imshow('Head Pose + Local YOLO Detection', frame)
            if cv2.waitKey(1) & 0xFF == 27:  # ESC é€€å‡º
                break
            continue                         # è·³åˆ°ä¸‹ä¸€å¸§

        # ---------- 1. å§¿æ€ä¸åˆ†å‰² ----------
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pose_results = pose.process(rgb_frame)                # å§¿æ€æ¨ç†
        seg_results = selfie_segmentation.process(rgb_frame)  # äººä½“åˆ†å‰²
        mask = seg_results.segmentation_mask > 0.5            # True=äººä½“åƒç´ 
        if not pose_results.pose_landmarks:
            print("No pose landmarks detected.")
        # ---------- 2. ä»…åœ¨æ£€æµ‹åˆ°å§¿æ€æ—¶ç»§ç»­ ----------
        if pose_results.pose_landmarks:
            # --------  å…³é”®ç‚¹åæ ‡ --------
            landmarks = pose_results.pose_landmarks.landmark
            nose      = landmarks[mp_pose.PoseLandmark.NOSE]
            left_ear  = landmarks[mp_pose.PoseLandmark.LEFT_EAR]
            right_ear = landmarks[mp_pose.PoseLandmark.RIGHT_EAR]

            nose_coords      = (int(nose.x * w),      int(nose.y * h))
            left_ear_coords  = (int(left_ear.x * w),  int(left_ear.y * h))
            right_ear_coords = (int(right_ear.x * w), int(right_ear.y * h))
            

            # --------  å¤´éƒ¨æ–¹å‘å‘é‡ --------
            ear_midpoint = (                           # ä¸¤è€³ä¸­ç‚¹
                (left_ear_coords[0] + right_ear_coords[0]) // 2,
                (left_ear_coords[1] + right_ear_coords[1]) // 2
            )
            direction_vector = (                       # é¼»å­æŒ‡å‘å‘é‡
                nose_coords[0] - ear_midpoint[0],
                nose_coords[1] - ear_midpoint[1]
            )

            # -------- å‘é‡å½’ä¸€åŒ– --------
            norm = np.sqrt(direction_vector[0]**2 + direction_vector[1]**2)
            vec_length_threshold = 15                  # æ­£è§†é˜ˆå€¼ (åƒç´ )
            if norm == 0:                              # é¿å…é™¤é›¶
                continue
            unit_vector = (direction_vector[0] / norm,
                           direction_vector[1] / norm)
            
            # ---------- 3. è®¡ç®— ROIï¼ˆå¤´éƒ¨æœå‘çŸ©å½¢ï¼‰ ----------
            roi_length = 4000                        # é‡‡æ ·çº¿é•¿åº¦ (åƒç´ )
            samples = 60                               # é‡‡æ ·ç‚¹æ•°
            inside = 0  
            roi_width = 100                            # ROI å®½åº¦ (åƒç´ )
            # ROI ä¸­å¿ƒç‚¹ = é¼»å­æ²¿æœå‘ç§»åŠ¨ roi_length/2
            roi_center = (
                int(nose_coords[0] + unit_vector[0] * roi_length / 2),
                int(nose_coords[1] + unit_vector[1] * roi_length / 2)
            )
            # ROI é•¿è¾¹å‘é‡ (dx, dy)  & å‚ç›´æ–¹å‘å‘é‡
            dx = int(unit_vector[0] * roi_length / 2)
            dy = int(unit_vector[1] * roi_length / 2)
            perp_vector = (-unit_vector[1], unit_vector[0])   # é€†æ—¶é’ˆ 90Â°
            px = int(perp_vector[0] * roi_width / 2)
            py = int(perp_vector[1] * roi_width / 2)

            # ROI å››ä¸ªé¡¶ç‚¹
            pt1 = (roi_center[0] - dx - px, roi_center[1] - dy - py)
            pt2 = (roi_center[0] + dx - px, roi_center[1] + dy - py)
            pt3 = (roi_center[0] + dx + px, roi_center[1] + dy + py)
            pt4 = (roi_center[0] - dx + px, roi_center[1] - dy + py)

            # ----------  å¯è§†åŒ– ----------
            frame[mask] = (0, 255, 0)                  # äººä½“æ©ç æ¶‚ç»¿è‰²
            cv2.polylines(
                frame, [np.array([pt1, pt2, pt3, pt4], np.int32)],
                isClosed=True, color=(0, 255, 255), thickness=2
            )
            # å¤´éƒ¨æ–¹å‘çº¿ & å…³é”®ç‚¹
            cv2.line(frame, nose_coords,
                     (int(nose_coords[0] + direction_vector[0] * 200),
                      int(nose_coords[1] + direction_vector[1] * 200)),
                     (0, 255, 255), 2)
            cv2.circle(frame, nose_coords, 5, (0, 255, 0), -1)
            cv2.circle(frame, left_ear_coords, 5, (255, 0, 0), -1)
            cv2.circle(frame, right_ear_coords, 5, (255, 0, 0), -1)
            
            # -------- æ¡ä»¶â‘ ï¼šé¢éƒ¨æ­£è§†æ‘„åƒå¤´ --------
            if norm < vec_length_threshold:
                cv2.putText(
                    frame, "Face Forward: YOLO Skipped", (50, 80),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2
                )
                cv2.imshow('Head Pose + Local YOLO Detection', frame)
                if cv2.waitKey(1) & 0xFF == 27:
                    break
                continue                               # è·³è¿‡ YOLO

            # -------- 2.5 æ¡ä»¶â‘¡ï¼šé¼»å­æŒ‡å‘çº¿ä»åœ¨äººä½“å†… --------
                               # è½åœ¨äººä½“å†…è®¡æ•°
            roi_length_check = 100            # åªå– 40 cmï¼ˆæˆ–åƒç´  400ï¼‰æ¥åˆ¤å®šå³å¯
            samples = 40                      # é‡‡æ ·ç‚¹æ•°
            inside = 0                        # â† æ¯å¸§éƒ½è¦å½’é›¶

            for i in range(1, samples + 1):   # (0, 1] ç­‰é—´è·
                ratio = i / samples
                sx = int(nose_coords[0] + unit_vector[0] * roi_length_check * ratio)
                sy = int(nose_coords[1] + unit_vector[1] * roi_length_check * ratio)
                if 0 <= sx < w and 0 <= sy < h and mask[sy, sx]:
                    inside += 1

            # è‹¥è¶…è¿‡ 70 % çš„é‡‡æ ·ç‚¹åœ¨äººä½“å†… â†’ è·³è¿‡ YOLO
            if inside / samples > 0.7:
                cv2.putText(frame, "Line Inside Body: YOLO Skipped",
                            (50, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                cv2.imshow('Head Pose + Local YOLO Detection', frame)
                if cv2.waitKey(1) & 0xFF == 27:  # ESC å¯ä»¥æå‰é€€å‡º
                    break
                continue          # â† ç›´æ¥è¿›å…¥ while ä¸‹ä¸€å¸§
                     # è·³è¿‡ YOLO
            
            # ---------- 6. YOLO å…¨å›¾æ¨ç† ----------
            results_yolo = model(frame, verbose=False)

            for r in results_yolo:
                for box in r.boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = float(box.conf[0].cpu().numpy())
                    cls = int(box.cls[0].cpu().numpy())
                    label = model.names[cls]

                    # è®¡ç®— bbox ä¸­å¿ƒç‚¹
                    cx = (x1 + x2) / 2
                    cy = (y1 + y2) / 2
                    center_point = (int(cx), int(cy))

                    # åˆ¤æ–­ä¸­å¿ƒç‚¹æ˜¯å¦åœ¨ ROI åŒºåŸŸå†…ï¼ˆæœå‘çº¿æ„æˆçš„å¤šè¾¹å½¢ï¼‰
                    roi_poly = np.array([pt1, pt2, pt3, pt4], np.int32)
                    is_inside = cv2.pointPolygonTest(roi_poly, center_point, measureDist=False)

                    if is_inside < 0:  # ä¸åœ¨ ROI åŒºåŸŸå†…
                        continue

                    # âœ… å¦‚æœåœ¨ ROI ä¸­ï¼Œæ‰§è¡Œå¯è§†åŒ–å’Œè®°å½•é€»è¾‘
                    label_times[label] = current_time

                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)
                    cv2.putText(frame, f"{label} {conf:.2f}",
                                (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,
                                (0, 0, 255), 2)


            # ---------- 7. ç»´æŠ¤æ»‘åŠ¨çª—å£å†…æ ‡ç­¾ ----------
            detected_labels = {
                lbl for lbl, t in label_times.items()
                if current_time - t <= window_seconds and lbl != 'person'
            }
            print("Appeared labels:", detected_labels)

            # å¦‚æœéœ€è¦ LLM åˆ†æï¼Œå¯æ”¹ç”¨ label_analyzer.analyze_labels
            cooking_labels = detected_labels

            # ---------- 8. ç»˜åˆ¶æ ‡ç­¾åˆ—è¡¨ ----------
            if cooking_labels:
                cooking_text = "Cooking Items: " + ", ".join(cooking_labels)
                cv2.putText(frame, cooking_text, (50, 150),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
        cv2.imshow('Head Pose + Local YOLO Detection', frame) # to show the frame with all bboxes, texts and masks
        if cv2.waitKey(1) & 0xFF == 27: 
            break

        # ---------- 10. TTS / STT äº’åŠ¨ ----------
        if (current_time - last_tts_time > tts_interval) and detected_labels:
            tts_stt_in_progress = True
            cooking_list = list(detected_labels)
            cooking_str = ", ".join(cooking_list)

            # ----- 10.1 ä»…ä¸€ä¸ªç‰©ä½“ -----
            if len(cooking_list) == 1:
                tts_text = f"Are you looking for {cooking_str}?"
                play_text_to_speech(tts_text, language='en')
                stt_text = transcriber.auto_record_and_transcribe(5)
                print(f"ğŸ“ STT Result: {stt_text}")
                last_tts_time = time.time()
                tts_stt_in_progress = False

                if stt_text and ("yes" in stt_text.lower() or
                                 cooking_list[0].lower() in stt_text.lower()):
                    play_text_to_speech(
                        "OKï¼",
                        language='en'
                    )
                    last_query_result = f'please give me "{cooking_list[0]}"'
                else:
                    last_query_result =""

            # ----- 10.2 å¤šä¸ªç‰©ä½“ -----
            else:
                tts_text = f"There are {cooking_str}. What do you want?"
                play_text_to_speech(tts_text, language='en')
                stt_text = transcriber.auto_record_and_transcribe(5)
                print(f"ğŸ“ STT Result: {stt_text}")
                last_tts_time = time.time()
                tts_stt_in_progress = False

                found = ""
                if stt_text:
                    stt_lower = stt_text.lower()
                    for item in cooking_list:
                        if item.lower() in stt_lower:
                            found = item
                            break
                if found:
                    last_query_result = f'please give me "{found}"'
                    play_text_to_speech(
                        "OKï¼",
                        language='en'
                    )
                else:
                    last_query_result = ""

            print(last_query_result)
            # âœ… å†™å…¥ transcription.txtï¼ˆè®©ä¸»çº¿ç¨‹èƒ½è¯»åˆ°ï¼‰
            TRANS_FILE = "src/transcribe/transcription.txt"
            with open(TRANS_FILE, "w", encoding="utf-8") as f:
                f.write(last_query_result)

    # ===== å¾ªç¯ç»“æŸï¼Œé‡Šæ”¾èµ„æº =====
    capture.release()
    cv2.destroyAllWindows()


# ---------------- ä¸»ç¨‹åºå…¥å£ ----------------
if __name__ == "__main__":
    intention_predict('yolo_model/yolo11m.pt')
